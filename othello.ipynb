{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73fd443",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "    @keyframes lavaEffect {\n",
    "        0% { color: #00FFFF; } /* Cyan */\n",
    "        25% { color: #0000CD; } /* MediumBlue */\n",
    "        50% { color: #1E90FF; } /* DodgerBlue */\n",
    "        75% { color: #00BFFF; } /* DeepSkyBlue */\n",
    "        100% { color: #00FFFF; } /* Cyan */\n",
    "    }\n",
    "\n",
    "    #lava-text h1 {\n",
    "        text-align: center;\n",
    "        animation: lavaEffect 5s infinite;\n",
    "        text-shadow: 0 0 10px #00FFFF, 0 0 20px #0000CD, 0 0 30px #1E90FF, 0 0 40px #00BFFF, 0 0 50px #00FFFF; /* Adding text shadow for lava glow effect */\n",
    "        font-weight: bold;\n",
    "        font-family: inherit; /* Inherit the font-family from the parent */\n",
    "        font-size: 32px; /* Adjust the font-size as needed */\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div id=\"lava-text\">\n",
    "    <h1>The Strategic Heuristic Algorithm with Zero-Human Advancement for Mastering Othello</h1>\n",
    "</div>\n",
    "\n",
    "\n",
    "<h3 style=\"color: darkblue; font-weight: bold;\" >Introduction :</h3>\n",
    "\n",
    "This notebook presents the development and evaluation of a Strategic Heuristic Algorithm with Zero-Human Advancement (SHA-ZHA) for mastering the game of Othello. \n",
    "\n",
    "The traditional board game Othello, sometimes referred to as Reversi, was invented in 1883 by Lewis Waterman and John W. Mollett. It developed into a contemporary version with a fixed basic board configuration over time. After being invented in Japan in 1971 by Goro Hasegawa, Othello has gained worldwide recognition since its creation and has been a mainstay in competitive tournaments since 1977.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/20/Othello-Standard-Board.jpg/500px-Othello-Standard-Board.jpg\" alt=\"Othello Board\" width=\"400\" style=\"border-radius: 20px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "The strategic depth of this two-player game poses a significant challenge for artificial intelligence, primarily due to its vast state space. Although the exact complexity remains elusive, estimates range from 10^28 to 10^32, a magnitude surpassing 100 times the number of bacteria on Earth and 10^13 times the number of grains of sand on Earth. To appreciate this scale, envision the hypothetical scenario of generating one billion Othello positions per second, a task demanding over 10^18 years—70,000 times the age of the universe.\n",
    "\n",
    "Inspired by [DeepMind's AlphaZero](https://arxiv.org/abs/1712.01815), which achieved superhuman performance in chess, shogi, and Go through self-play and reinforcement learning, this work employs similar principles to master Othello. SHA-ZHA makes use of parallel game simulation and [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347) to allow the agent to learn and adapt through self-play without requiring any prior human understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819736f5-efcb-46c6-9385-d04ad9e6088b",
   "metadata": {
    "id": "819736f5-efcb-46c6-9385-d04ad9e6088b",
    "outputId": "a1507b1a-1f22-4b2b-bd1a-4fcf25046139"
   },
   "outputs": [],
   "source": [
    "! pip install torchsummary tqdm tabulate matplotlib numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93abcef",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkblue; font-weight: bold;\" >Package Descriptions :</h3>\n",
    "\n",
    "- **torchsummary**: A package that provides a summary of the layers in a PyTorch model, similar to `model.summary()` in Keras.\n",
    "  \n",
    "- **tqdm**: A package for displaying progress bars in loops and other iterators, making it easy to track the progress of long-running operations.\n",
    "\n",
    "- **tabulate**: A package for pretty-printing tabular data in various formats, including plain text, HTML, and LaTeX.\n",
    "\n",
    "- **matplotlib**: A comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    "\n",
    "- **numba**: A Just-In-Time (JIT) compiler that translates a subset of Python and NumPy code into fast machine code, enabling high-performance operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48e67e1c-be8f-4d40-b9e2-c9315fbcce67",
   "metadata": {
    "id": "48e67e1c-be8f-4d40-b9e2-c9315fbcce67"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import  DataLoader\n",
    "import torch.multiprocessing as mp\n",
    "import copy\n",
    "from _othello import OthelloRL,miniMax,OthelloBoard,asyncSelfPlay,parallelSelfPlay,PPO_Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d60037",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkblue; font-weight: bold;\">Learning Rate Scheduler, Torch Multiprocessing, and `_othello` Module</h3>\n",
    "\n",
    "<h5 style=\"color: darkblue; font-weight: bold;\">Learning Rate Scheduler</h5>\n",
    "The learning rate scheduler is a tool in PyTorch that adjusts the learning rate during training. Adjusting the learning rate is a common practice in reinforcement learning which can help improve the performance and convergence of the model. One common scheduler is `StepLR`, which decays the learning rate by a factor every few epochs. This helps in fine-tuning the learning process, preventing the model from overshooting minima and stabilizing the training process.\n",
    "\n",
    "<h5 style=\"color: darkblue; font-weight: bold;\">Torch Multiprocessing</h5>\n",
    "Torch multiprocessing allows for parallel processing in PyTorch, enabling faster and more efficient training, especially on multi-core processors. By using `torch.multiprocessing`, you can distribute the workload across multiple CPU cores or GPU devices. This is particularly useful for tasks like data loading and model training, where computational efficiency and speed are critical.\n",
    "\n",
    "<h5 style=\"color: darkblue; font-weight: bold;\">`_othello` Module</h5>\n",
    "The `_othello` module is a comprehensive implementation for training an agent to master the game of Othello. It includes several components:\n",
    "\n",
    "- **Proximal Policy Optimization (PPO)**: An advanced reinforcement learning algorithm developed by OpenAI. PPO balances exploration and exploitation by clipping probability ratios. This helps in stabilizing training and improving the performance of the agent. **The Proximal Policy Optimization (PPO) loss** consists of three components:\n",
    "\n",
    "$$\n",
    "\\text{PPO Loss} = \\text{Policy Loss} + \\text{Value Loss} + \\text{Entropy Loss}\n",
    "$$\n",
    "<font color='green' size='2'>\n",
    "\n",
    "1. **Policy Loss**: Measures the difference between the predicted action probabilities and the action probabilities that maximize the expected return.\n",
    "\n",
    "2. **Value Loss**: Measures the difference between the predicted value function and the observed returns.\n",
    "\n",
    "3. **Entropy Loss**: Measures the uncertainty or randomness in the agent's policy.\n",
    "\n",
    "</font>\n",
    "  \n",
    "- **Multiprocessing**: The module uses multiprocessing for parallel training, making the implementation highly efficient. This involves running multiple instances of self-play in parallel, significantly speeding up the learning process.\n",
    "  \n",
    "- **Othello Game Implementation**: A robust implementation of the Othello game, allowing for various configurations and scenarios for training the agent.\n",
    "  \n",
    "- **Minimax Engine**: The module includes a minimax engine to measure the performance of the trained agent. The minimax algorithm is a classical AI strategy for decision-making in two-player games, providing a baseline for evaluating the agent's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49194f9c-3bb7-41c9-beeb-59f6e2bc032a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49194f9c-3bb7-41c9-beeb-59f6e2bc032a",
    "outputId": "eb5e2c54-53c8-46ea-a199-736237f73075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**discout factor is :  0.9999306877, LR reaches 50% in 20000 steps.\n",
      "number of games per step : 4096\n",
      "randomness seed : 1034430421326500,numpy : 2724217580.\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.5e-4 # learning rate\n",
    "steps = 6000 # number of steps\n",
    "batch_size = 256 #batch size\n",
    "test_num = 250 # games played in each test point\n",
    "test_frq = 25 # test every test_frq of steps\n",
    "game_per_worker = 512 # number of games played in each step by one worker\n",
    "num_workers = 16 # number of workers in parallel\n",
    "epochs_per_step = 1 # number of epochs per step\n",
    "board_base_size = 8 # base size of the board\n",
    "num_boards = 1 # number of board sizes\n",
    "load_check_point = False # load last check point\n",
    "current_step = 1 # current step\n",
    "path = \"/checkpoint/\" # path to save the results\n",
    "epsilon_max = 0.05 # maximum value of epsilon\n",
    "epsilon_min = 0.0 # minimum value of epsilon\n",
    "scale_fac = 4 # control the rate of decay of Epsilon\n",
    "engine_depth = 7 #depth of minimax engine\n",
    "########################################\n",
    "#model architecture\n",
    "internal_channels = 64 # number of internal channels\n",
    "policy_net_depth = 8 # depth of the policy network\n",
    "########################################\n",
    "# PPO loss parameters\n",
    "value_coefficient = 0.5 # value coefficient\n",
    "entropy_coefficient = 0.09 # entropy coefficient\n",
    "clip_param = 0.2 # clip parameter\n",
    "########################################\n",
    "gamma_lr = np.exp(np.log(1/8)/(steps))# reduction factor for learning rate\n",
    "# np.exp(np.log(0.5)/ x ) reach to 50% after x\n",
    "print(f\"**discout factor is : {gamma_lr : .010f}, LR reaches 50% in {int(np.log(0.8)/np.log(gamma_lr))} steps.\")\n",
    "step_lr = 1 # step size for the learning rate\n",
    "ref_path = \"/workspace/shaza_old.pth\" # path to reference model\n",
    "print(f\"number of games per step : {game_per_worker*num_workers}\")\n",
    "print(f\"random seed : { torch.initial_seed()} ,numpy : {np.random.get_state()[1][0]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819cbeff",
   "metadata": {},
   "source": [
    "- Considering an estimated duration of 5 minutes for each Othello game, a single step utilizes 16 parallel processes for simulation. Each of these processes generates 512 games, resulting in a total of 8192 games per step. Therefore, every step is equivalent to approximately 28.4 days of continuous gameplay in human terms.\n",
    "- With a total of 6000 steps in the training process, the agent will experience an equivalent of approximately 467.27 years of gameplay in human terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f754a31",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkblue; font-weight: bold;\">Agent Network Architecture</h3>\n",
    "\n",
    "The agent consists of two deep convolutional neural networks based on ConvNeXt. The first network is the policy network, which maps different boards into a probability distribution over actions. The second network is the value network, which provides an evaluation of the board.\n",
    "\n",
    "<h5 style=\"color: darkblue; font-weight: bold;\">ConvNeXt</h5>\n",
    "ConvNeXt is a modern architecture for convolutional neural networks inspired by the design principles of vision transformers (ViTs). It combines the strengths of convolutional layers and transformer-like features, achieving state-of-the-art performance in various image recognition tasks. ConvNeXt introduces innovations such as:\n",
    "\n",
    "- **Depthwise Convolutions**: These reduce the number of parameters and computations, making the network more efficient.\n",
    "- **Layer Normalization**: This stabilizes and accelerates training by normalizing the inputs across the feature map dimensions.\n",
    "- **Residual Connections**: These help in training deeper networks by mitigating the vanishing gradient problem.\n",
    "\n",
    "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTTm384qquw_hbC0UIIhF8Jnr9gHvtNokBOCQ&s\" alt=\"ConvNeXt\" hight=\"250\" style=\"display: block; margin-left: auto; margin-right: auto; border-radius: 15px;\"/>\n",
    "\n",
    "<h5 style=\"color: darkblue; font-weight: bold;\">Convolutional Block Attention Module (CBAM)</h5>\n",
    "Another key component that significantly speeds up the learning process is the Convolutional Block Attention Module (CBAM). CBAM enhances the feature representation of the neural network by focusing on important information and suppressing irrelevant details. It consists of two sequential sub-modules:\n",
    "\n",
    "- **Channel Attention Module**: This emphasizes informative channels and suppresses less useful ones by computing channel-wise attention.\n",
    "- **Spatial Attention Module**: This enhances important spatial features and suppresses irrelevant ones by computing spatial attention maps.\n",
    "\n",
    "By applying both channel and spatial attention, CBAM improves the network's ability to focus on crucial parts of the input, leading to better performance and faster convergence.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*cvZx6H1aDsSgqQ1z\" alt=\"CBAM\" width=\"600\" style=\"display: block; margin-left: auto; margin-right: auto; border-radius: 15px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d43f4d99-e5d3-4b41-9c6f-1a8223c62884",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d43f4d99-e5d3-4b41-9c6f-1a8223c62884",
    "outputId": "fa8a07b4-0b4c-40b2-bcc1-381d136c65f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current device is: cuda.\n",
      "Model A wasn't found : [Errno 2] No such file or directory: '/checkpoint/shaza_old.pth'\n",
      "summary of model : new\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [-1, 64, 8, 8]           1,152\n",
      "         GroupNorm-2             [-1, 64, 8, 8]             128\n",
      "         LeakyReLU-3             [-1, 64, 8, 8]               0\n",
      "          Identity-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5            [-1, 256, 8, 8]         147,456\n",
      "         GroupNorm-6            [-1, 256, 8, 8]             512\n",
      " AdaptiveAvgPool2d-7            [-1, 256, 1, 1]               0\n",
      "            Conv2d-8             [-1, 32, 1, 1]           8,224\n",
      "         LeakyReLU-9             [-1, 32, 1, 1]               0\n",
      "           Conv2d-10            [-1, 256, 1, 1]           8,448\n",
      "          Sigmoid-11            [-1, 256, 1, 1]               0\n",
      "AdaptiveMaxPool2d-12            [-1, 256, 1, 1]               0\n",
      "           Conv2d-13             [-1, 32, 1, 1]           8,224\n",
      "        LeakyReLU-14             [-1, 32, 1, 1]               0\n",
      "           Conv2d-15            [-1, 256, 1, 1]           8,448\n",
      "          Sigmoid-16            [-1, 256, 1, 1]               0\n",
      " ChannelAttention-17            [-1, 256, 1, 1]               0\n",
      "           Conv2d-18              [-1, 1, 8, 8]              18\n",
      "          Sigmoid-19              [-1, 1, 8, 8]               0\n",
      " SpatialAttention-20              [-1, 1, 8, 8]               0\n",
      "             CBAM-21            [-1, 256, 8, 8]               0\n",
      "           Conv2d-22            [-1, 256, 8, 8]          16,640\n",
      "        LeakyReLU-23            [-1, 256, 8, 8]               0\n",
      "           Conv2d-24             [-1, 64, 8, 8]          16,448\n",
      "    ResidualBlock-25             [-1, 64, 8, 8]               0\n",
      "         Identity-26             [-1, 64, 8, 8]               0\n",
      "           Conv2d-27            [-1, 256, 8, 8]         147,456\n",
      "        GroupNorm-28            [-1, 256, 8, 8]             512\n",
      "AdaptiveAvgPool2d-29            [-1, 256, 1, 1]               0\n",
      "           Conv2d-30             [-1, 32, 1, 1]           8,224\n",
      "        LeakyReLU-31             [-1, 32, 1, 1]               0\n",
      "           Conv2d-32            [-1, 256, 1, 1]           8,448\n",
      "          Sigmoid-33            [-1, 256, 1, 1]               0\n",
      "AdaptiveMaxPool2d-34            [-1, 256, 1, 1]               0\n",
      "           Conv2d-35             [-1, 32, 1, 1]           8,224\n",
      "        LeakyReLU-36             [-1, 32, 1, 1]               0\n",
      "           Conv2d-37            [-1, 256, 1, 1]           8,448\n",
      "          Sigmoid-38            [-1, 256, 1, 1]               0\n",
      " ChannelAttention-39            [-1, 256, 1, 1]               0\n",
      "           Conv2d-40              [-1, 1, 8, 8]              18\n",
      "          Sigmoid-41              [-1, 1, 8, 8]               0\n",
      " SpatialAttention-42              [-1, 1, 8, 8]               0\n",
      "             CBAM-43            [-1, 256, 8, 8]               0\n",
      "           Conv2d-44            [-1, 256, 8, 8]          16,640\n",
      "        LeakyReLU-45            [-1, 256, 8, 8]               0\n",
      "           Conv2d-46             [-1, 64, 8, 8]          16,448\n",
      "    ResidualBlock-47             [-1, 64, 8, 8]               0\n",
      "         Identity-48             [-1, 64, 8, 8]               0\n",
      "           Conv2d-49            [-1, 256, 8, 8]         147,456\n",
      "        GroupNorm-50            [-1, 256, 8, 8]             512\n",
      "AdaptiveAvgPool2d-51            [-1, 256, 1, 1]               0\n",
      "           Conv2d-52             [-1, 32, 1, 1]           8,224\n",
      "        LeakyReLU-53             [-1, 32, 1, 1]               0\n",
      "           Conv2d-54            [-1, 256, 1, 1]           8,448\n",
      "          Sigmoid-55            [-1, 256, 1, 1]               0\n",
      "AdaptiveMaxPool2d-56            [-1, 256, 1, 1]               0\n",
      "           Conv2d-57             [-1, 32, 1, 1]           8,224\n",
      "        LeakyReLU-58             [-1, 32, 1, 1]               0\n",
      "           Conv2d-59            [-1, 256, 1, 1]           8,448\n",
      "          Sigmoid-60            [-1, 256, 1, 1]               0\n",
      " ChannelAttention-61            [-1, 256, 1, 1]               0\n",
      "           Conv2d-62              [-1, 1, 8, 8]              18\n",
      "          Sigmoid-63              [-1, 1, 8, 8]               0\n",
      " SpatialAttention-64              [-1, 1, 8, 8]               0\n",
      "             CBAM-65            [-1, 256, 8, 8]               0\n",
      "           Conv2d-66            [-1, 256, 8, 8]          16,640\n",
      "        LeakyReLU-67            [-1, 256, 8, 8]               0\n",
      "           Conv2d-68             [-1, 64, 8, 8]          16,448\n",
      "    ResidualBlock-69             [-1, 64, 8, 8]               0\n",
      "         Identity-70             [-1, 64, 8, 8]               0\n",
      "           Conv2d-71            [-1, 256, 8, 8]         147,456\n",
      "        GroupNorm-72            [-1, 256, 8, 8]             512\n",
      "AdaptiveAvgPool2d-73            [-1, 256, 1, 1]               0\n",
      "           Conv2d-74             [-1, 32, 1, 1]           8,224\n",
      "        LeakyReLU-75             [-1, 32, 1, 1]               0\n",
      "           Conv2d-76            [-1, 256, 1, 1]           8,448\n",
      "          Sigmoid-77            [-1, 256, 1, 1]               0\n",
      "AdaptiveMaxPool2d-78            [-1, 256, 1, 1]               0\n",
      "           Conv2d-79             [-1, 32, 1, 1]           8,224\n",
      "        LeakyReLU-80             [-1, 32, 1, 1]               0\n",
      "           Conv2d-81            [-1, 256, 1, 1]           8,448\n",
      "          Sigmoid-82            [-1, 256, 1, 1]               0\n",
      " ChannelAttention-83            [-1, 256, 1, 1]               0\n",
      "           Conv2d-84              [-1, 1, 8, 8]              18\n",
      "          Sigmoid-85              [-1, 1, 8, 8]               0\n",
      " SpatialAttention-86              [-1, 1, 8, 8]               0\n",
      "             CBAM-87            [-1, 256, 8, 8]               0\n",
      "           Conv2d-88            [-1, 256, 8, 8]          16,640\n",
      "        LeakyReLU-89            [-1, 256, 8, 8]               0\n",
      "           Conv2d-90             [-1, 64, 8, 8]          16,448\n",
      "    ResidualBlock-91             [-1, 64, 8, 8]               0\n",
      "         Identity-92             [-1, 64, 8, 8]               0\n",
      "           Conv2d-93            [-1, 256, 8, 8]         147,456\n",
      "        GroupNorm-94            [-1, 256, 8, 8]             512\n",
      "AdaptiveAvgPool2d-95            [-1, 256, 1, 1]               0\n",
      "           Conv2d-96             [-1, 32, 1, 1]           8,224\n",
      "        LeakyReLU-97             [-1, 32, 1, 1]               0\n",
      "           Conv2d-98            [-1, 256, 1, 1]           8,448\n",
      "          Sigmoid-99            [-1, 256, 1, 1]               0\n",
      "AdaptiveMaxPool2d-100            [-1, 256, 1, 1]               0\n",
      "          Conv2d-101             [-1, 32, 1, 1]           8,224\n",
      "       LeakyReLU-102             [-1, 32, 1, 1]               0\n",
      "          Conv2d-103            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-104            [-1, 256, 1, 1]               0\n",
      "ChannelAttention-105            [-1, 256, 1, 1]               0\n",
      "          Conv2d-106              [-1, 1, 8, 8]              18\n",
      "         Sigmoid-107              [-1, 1, 8, 8]               0\n",
      "SpatialAttention-108              [-1, 1, 8, 8]               0\n",
      "            CBAM-109            [-1, 256, 8, 8]               0\n",
      "          Conv2d-110            [-1, 256, 8, 8]          16,640\n",
      "       LeakyReLU-111            [-1, 256, 8, 8]               0\n",
      "          Conv2d-112             [-1, 64, 8, 8]          16,448\n",
      "   ResidualBlock-113             [-1, 64, 8, 8]               0\n",
      "        Identity-114             [-1, 64, 8, 8]               0\n",
      "          Conv2d-115            [-1, 256, 8, 8]         147,456\n",
      "       GroupNorm-116            [-1, 256, 8, 8]             512\n",
      "AdaptiveAvgPool2d-117            [-1, 256, 1, 1]               0\n",
      "          Conv2d-118             [-1, 32, 1, 1]           8,224\n",
      "       LeakyReLU-119             [-1, 32, 1, 1]               0\n",
      "          Conv2d-120            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-121            [-1, 256, 1, 1]               0\n",
      "AdaptiveMaxPool2d-122            [-1, 256, 1, 1]               0\n",
      "          Conv2d-123             [-1, 32, 1, 1]           8,224\n",
      "       LeakyReLU-124             [-1, 32, 1, 1]               0\n",
      "          Conv2d-125            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-126            [-1, 256, 1, 1]               0\n",
      "ChannelAttention-127            [-1, 256, 1, 1]               0\n",
      "          Conv2d-128              [-1, 1, 8, 8]              18\n",
      "         Sigmoid-129              [-1, 1, 8, 8]               0\n",
      "SpatialAttention-130              [-1, 1, 8, 8]               0\n",
      "            CBAM-131            [-1, 256, 8, 8]               0\n",
      "          Conv2d-132            [-1, 256, 8, 8]          16,640\n",
      "       LeakyReLU-133            [-1, 256, 8, 8]               0\n",
      "          Conv2d-134             [-1, 64, 8, 8]          16,448\n",
      "   ResidualBlock-135             [-1, 64, 8, 8]               0\n",
      "        Identity-136             [-1, 64, 8, 8]               0\n",
      "          Conv2d-137            [-1, 256, 8, 8]         147,456\n",
      "       GroupNorm-138            [-1, 256, 8, 8]             512\n",
      "AdaptiveAvgPool2d-139            [-1, 256, 1, 1]               0\n",
      "          Conv2d-140             [-1, 32, 1, 1]           8,224\n",
      "       LeakyReLU-141             [-1, 32, 1, 1]               0\n",
      "          Conv2d-142            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-143            [-1, 256, 1, 1]               0\n",
      "AdaptiveMaxPool2d-144            [-1, 256, 1, 1]               0\n",
      "          Conv2d-145             [-1, 32, 1, 1]           8,224\n",
      "       LeakyReLU-146             [-1, 32, 1, 1]               0\n",
      "          Conv2d-147            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-148            [-1, 256, 1, 1]               0\n",
      "ChannelAttention-149            [-1, 256, 1, 1]               0\n",
      "          Conv2d-150              [-1, 1, 8, 8]              18\n",
      "         Sigmoid-151              [-1, 1, 8, 8]               0\n",
      "SpatialAttention-152              [-1, 1, 8, 8]               0\n",
      "            CBAM-153            [-1, 256, 8, 8]               0\n",
      "          Conv2d-154            [-1, 256, 8, 8]          16,640\n",
      "       LeakyReLU-155            [-1, 256, 8, 8]               0\n",
      "          Conv2d-156             [-1, 64, 8, 8]          16,448\n",
      "   ResidualBlock-157             [-1, 64, 8, 8]               0\n",
      "        Identity-158             [-1, 64, 8, 8]               0\n",
      "          Conv2d-159            [-1, 256, 8, 8]         147,456\n",
      "       GroupNorm-160            [-1, 256, 8, 8]             512\n",
      "AdaptiveAvgPool2d-161            [-1, 256, 1, 1]               0\n",
      "          Conv2d-162             [-1, 32, 1, 1]           8,224\n",
      "       LeakyReLU-163             [-1, 32, 1, 1]               0\n",
      "          Conv2d-164            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-165            [-1, 256, 1, 1]               0\n",
      "AdaptiveMaxPool2d-166            [-1, 256, 1, 1]               0\n",
      "          Conv2d-167             [-1, 32, 1, 1]           8,224\n",
      "       LeakyReLU-168             [-1, 32, 1, 1]               0\n",
      "          Conv2d-169            [-1, 256, 1, 1]           8,448\n",
      "         Sigmoid-170            [-1, 256, 1, 1]               0\n",
      "ChannelAttention-171            [-1, 256, 1, 1]               0\n",
      "          Conv2d-172              [-1, 1, 8, 8]              18\n",
      "         Sigmoid-173              [-1, 1, 8, 8]               0\n",
      "SpatialAttention-174              [-1, 1, 8, 8]               0\n",
      "            CBAM-175            [-1, 256, 8, 8]               0\n",
      "          Conv2d-176            [-1, 256, 8, 8]          16,640\n",
      "       LeakyReLU-177            [-1, 256, 8, 8]               0\n",
      "          Conv2d-178             [-1, 64, 8, 8]          16,448\n",
      "   ResidualBlock-179             [-1, 64, 8, 8]               0\n",
      "          Conv2d-180              [-1, 1, 8, 8]              65\n",
      "         Softmax-181                   [-1, 64]               0\n",
      "          Conv2d-182             [-1, 32, 8, 8]             864\n",
      "       GroupNorm-183             [-1, 32, 8, 8]              64\n",
      "       LeakyReLU-184             [-1, 32, 8, 8]               0\n",
      "          Conv2d-185             [-1, 32, 8, 8]           9,216\n",
      "       GroupNorm-186             [-1, 32, 8, 8]              64\n",
      "       LeakyReLU-187             [-1, 32, 8, 8]               0\n",
      "          Conv2d-188             [-1, 32, 8, 8]           9,216\n",
      "       GroupNorm-189             [-1, 32, 8, 8]              64\n",
      "       LeakyReLU-190             [-1, 32, 8, 8]               0\n",
      "       MaxPool2d-191             [-1, 64, 4, 4]               0\n",
      "           Block-192             [-1, 64, 4, 4]               0\n",
      "          Conv2d-193             [-1, 64, 4, 4]          36,864\n",
      "       GroupNorm-194             [-1, 64, 4, 4]             128\n",
      "       LeakyReLU-195             [-1, 64, 4, 4]               0\n",
      "          Conv2d-196             [-1, 64, 4, 4]          36,864\n",
      "       GroupNorm-197             [-1, 64, 4, 4]             128\n",
      "       LeakyReLU-198             [-1, 64, 4, 4]               0\n",
      "       MaxPool2d-199            [-1, 128, 2, 2]               0\n",
      "           Block-200            [-1, 128, 2, 2]               0\n",
      "          Conv2d-201            [-1, 128, 2, 2]         147,456\n",
      "       GroupNorm-202            [-1, 128, 2, 2]             256\n",
      "       LeakyReLU-203            [-1, 128, 2, 2]               0\n",
      "          Conv2d-204            [-1, 128, 2, 2]         147,456\n",
      "       GroupNorm-205            [-1, 128, 2, 2]             256\n",
      "       LeakyReLU-206            [-1, 128, 2, 2]               0\n",
      "       MaxPool2d-207            [-1, 256, 1, 1]               0\n",
      "           Block-208            [-1, 256, 1, 1]               0\n",
      "         Flatten-209                  [-1, 256]               0\n",
      "          Linear-210                    [-1, 1]             257\n",
      "       value_Net-211                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 2,105,842\n",
      "Trainable params: 2,105,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 6.21\n",
      "Params size (MB): 8.03\n",
      "Estimated Total Size (MB): 14.28\n",
      "----------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Your current device is: {device}.\")\n",
    "#####################\n",
    "#Double Q-learning algorithm\n",
    "#Create 2 instances of OthelloRL\n",
    "agents = [OthelloRL(internal_channels= internal_channels,res_depth=policy_net_depth,name = \"new\",return_val = True),\n",
    "          OthelloRL(internal_channels= internal_channels,res_depth=policy_net_depth,name = \"old\",return_val = True)]\n",
    "####################\n",
    "if load_check_point:\n",
    "  try:\n",
    "    agents[0].load(os.path.join(path,f\"shaza_{agents[0].name}.pth\"))\n",
    "    agents[1].load(os.path.join(path,f\"shaza_{agents[1].name}.pth\"))\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "# load the reference model\n",
    "ref = OthelloRL()\n",
    "ref = ref.to(device)\n",
    "ref.load(path = ref_path)\n",
    "ref.name = \"refernce\"\n",
    "\n",
    "#best checkpoint\n",
    "best_model = copy.deepcopy(agents[0])\n",
    "best_model.name = \"best_model\"\n",
    "\n",
    "# set name\n",
    "# Move the agents to the device\n",
    "agents[0] = agents[0].to(device)\n",
    "agents[1] = agents[1].to(device)\n",
    "\n",
    "################\n",
    "agents[0].train()\n",
    "agents[1].eval()\n",
    "ref.eval() # set to evaluation mode\n",
    "# Define the loss function and optimizer\n",
    "criterion = PPO_Loss(clip_param = clip_param,value_coefficient = value_coefficient,entropy_coefficient = entropy_coefficient,echo=True)\n",
    "#######################################\n",
    "optim = torch.optim.AdamW(agents[0].parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "if load_check_point:\n",
    "    try:\n",
    "        optim.load_state_dict(torch.load(os.path.join(path,f\"optimizer.pth\"),map_location=device)[\"optimizer_state_dict\"])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# load scheduler\n",
    "\n",
    "scheduler = StepLR(optim,step_size= step_lr,gamma= gamma_lr)\n",
    "\n",
    "\n",
    "if load_check_point:\n",
    "    try:\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(path,f\"scheduler.pth\"),map_location=device)[\"scheduler_state_dict\"])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "# Lists to store training metrics\n",
    "win_trace = {f\"Agent_vs_Random\" : [], # store the win trace for each\n",
    "             f\"Agent_vs_Engine\":[],\n",
    "             f\"{agents[0].name}_vs_{best_model.name}\": [],\n",
    "             f\"Agent_vs_{ref.name}\" : [],\n",
    "             }\n",
    "loss = {\"total_loss\":[],\n",
    "        \"policy_loss\":[],\n",
    "        \"value_loss\":[],\n",
    "        \"entropy_loss\":[],\n",
    "        } # store loss form each epoch\n",
    "\n",
    "time_step = [] # store the time in sec for each epoch\n",
    "\n",
    "if load_check_point:\n",
    "    try :\n",
    "        data = np.load(os.path.join(path,\"log.npy\"),allow_pickle=True).item()\n",
    "        win_trace = data[\"win trace\"]\n",
    "        total_loss = data[\"loss\"]\n",
    "        time_step = data[\"avg time ber epoch\"]\n",
    "        current_step = data[\"step\"] + 1\n",
    "        print(\"check point has been loaded\")\n",
    "    except Exception as e :\n",
    "        print(e)\n",
    "\n",
    "\n",
    "print(f\"summary of model : {agents[0].name}\")\n",
    "summary(agents[0], [(2,8,8), (1,8,8)])\n",
    "print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "#print(f\"summary of model : {agents[1].name}\")\n",
    "#summary(agents[1], [(2,8,8), (1,8,8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d955b8a5",
   "metadata": {},
   "source": [
    "<h5 style=\"color: darkblue; font-weight: bold;\">Exploration Strategies</h5>\n",
    "To increase exploration and break possible biases generated from selecting actions with the highest probability, I included epsilon-greedy exploration, although Proximal Policy Optimization (PPO) already has Boltzmann exploration in place.\n",
    "\n",
    "- **Epsilon-Greedy Exploration**: In this strategy, with a small probability (epsilon), a random action is selected instead of the action with the highest probability. This helps in exploring the action space more thoroughly and avoids getting stuck in local optima.\n",
    "- **Boltzmann Exploration**: PPO uses this strategy to sample actions according to a probability distribution derived from the action values. This promotes exploration by allowing for a diverse range of actions based on their probabilities.\n",
    "\n",
    "Combining these exploration strategies ensures a balance between exploration and exploitation, leading to a more robust learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9225f343-29b1-4bf5-bc8f-88aeaa714b94",
   "metadata": {
    "id": "9225f343-29b1-4bf5-bc8f-88aeaa714b94"
   },
   "outputs": [],
   "source": [
    "def calcEpsilon(ep):\n",
    "\n",
    "    return (epsilon_max - epsilon_min)*np.exp(-1*(ep*scale_fac)/steps) + epsilon_min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501247d",
   "metadata": {},
   "source": [
    "Now, for testing lets stop Agents from returning value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28aaa8ad-1e17-4582-a183-f13b42876d82",
   "metadata": {
    "id": "28aaa8ad-1e17-4582-a183-f13b42876d82"
   },
   "outputs": [],
   "source": [
    "agents[0].return_val = False\n",
    "agents[1].return_val = False\n",
    "best_model.return_val = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "217ab3a8-fd6b-4d0d-a823-2618ccbdfe2c",
   "metadata": {
    "id": "217ab3a8-fd6b-4d0d-a823-2618ccbdfe2c"
   },
   "outputs": [],
   "source": [
    "def Test(agent_A,agent_B = None, test_num = test_num,board_base_size = board_base_size,num_boards = num_boards):\n",
    "        # model to evaluation mode\n",
    "        agent_A.cpu().eval()\n",
    "        agent_A.return_val = False\n",
    "        if agent_B :\n",
    "          agent_B.cpu().eval()\n",
    "          agent_B.return_val = False\n",
    "\n",
    "        score = np.array([0,0])\n",
    "\n",
    "        for i in tqdm(range(test_num),desc = f\"model {agent_A.name} vs { agent_B.name if agent_B else 'random' }\"):\n",
    "            # chance the size of the board from 8 to 10 to 12 and so on ...\n",
    "            board_size = board_base_size + 2*(i%num_boards) # dynamic board size\n",
    "            Board = OthelloBoard()\n",
    "            #########################################################\n",
    "            if agent_B :\n",
    "                score += Board.modelVsModel(agent_A,agent_B)\n",
    "            else :\n",
    "                score += Board.modelVsModel(agent_A,agent_A,Epsilon_2=1.0)\n",
    "\n",
    "        return score\n",
    "\n",
    "def RunTests(Q):\n",
    "        # testing the model agenst random and engine\n",
    "        #####################################################################\n",
    "        score_random = Test(Q)\n",
    "        win_trace['Agent_vs_Random'].append(round((score_random[0] / (score_random[0] + score_random[1])) * 100, 4))\n",
    "        # test model agenst pretrained reference model\n",
    "        ######################################################################\n",
    "        score_ref = Test(Q,ref, test_num = test_num)\n",
    "        win_trace[f\"Agent_vs_{ref.name}\"].append(round((score_ref[0] / (score_ref[0] + score_ref[1])) * 100, 4))\n",
    "        # test model agenst engine\n",
    "        #####################################################################\n",
    "        engine = miniMax(name = \"E\",depth = engine_depth)\n",
    "        score_ref = Test(Q,engine, test_num = test_num//2)\n",
    "        win_trace[f\"Agent_vs_Engine\"].append(round((score_ref[0] / (score_ref[0] + score_ref[1])) * 100, 4))\n",
    "\n",
    "\n",
    "        results = [\n",
    "            (f\"{Q.name} vs random\", f\"{win_trace['Agent_vs_Random'][-1]}%\"),\n",
    "            (f\"{Q.name} vs {ref.name}\", f\"{win_trace[f'Agent_vs_{ref.name}'][-1]}%\"),\n",
    "            (f\"{Q.name} vs {engine.name}\", f\"{win_trace[f'Agent_vs_Engine'][-1]}%\"),\n",
    "        ]\n",
    "\n",
    "        # Display table\n",
    "        table = tabulate(results, headers=[\"Comparison\", f\"Win Rate for {Q.name}\"])\n",
    "        print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b080db4-12fd-4cda-9fc8-f6f212ab55ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b080db4-12fd-4cda-9fc8-f6f212ab55ee",
    "outputId": "7acddcab-27d4-40a7-9bbf-13d8bab565d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started : self playing worker -0\n",
      "Started : self playing worker -1\n",
      "Started : self playing worker -2\n",
      "Started : self playing worker -3\n",
      "Finished : self playing worker -0\n",
      "Finished : self playing worker -1\n",
      "Finished : self playing worker -2\n",
      "Finished : self playing worker -3\n",
      "1652\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    result = parallelSelfPlay(model = agents[0],device = device)\n",
    "    print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c7e279",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkblue; font-weight: bold;\">Testing the Agent</h3>\n",
    "\n",
    "The baseline untrained agent will undergo a series of tests to evaluate its initial performance. The tests will be conducted against three different opponents:\n",
    "\n",
    "1. **Random Policy**: This opponent selects actions randomly without any strategic consideration. It serves as a basic benchmark to assess the untrained agent's performance against a completely unstructured strategy.\n",
    "   \n",
    "2. **Another Trained Agent**: This opponent is a version of the agent that has undergone training. Testing against a trained agent helps in understanding how the untrained agent fares against a more sophisticated and learned strategy.\n",
    "   \n",
    "3. **Minimax Algorithm**: This opponent uses the minimax algorithm with a selected depth to make decisions. The minimax algorithm is a classical AI strategy for decision-making in two-player games, which provides a more structured and competitive challenge. The selected depth determines the lookahead search depth of the algorithm, balancing between computational efficiency and strategic depth.\n",
    "\n",
    "Tests will continue during the training phase to continuously evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6adcae7-9615-48f8-be47-c18045524cf0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6adcae7-9615-48f8-be47-c18045524cf0",
    "outputId": "6a4815b7-3b6d-44fc-9430-505a6f16b64f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model new vs random:   0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model new vs random: 100%|██████████| 250/250 [00:55<00:00,  4.54it/s]\n",
      "model new vs refernce: 100%|██████████| 250/250 [01:21<00:00,  3.07it/s]\n",
      "model new vs E: 100%|██████████| 125/125 [00:58<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison       Win Rate for new\n",
      "---------------  ------------------\n",
      "new vs random    35.6846%\n",
      "new vs refernce  20.8333%\n",
      "new vs E         8.0645%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RunTests(agents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e703b",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkblue; font-weight: bold;\">Training the Model with PPO</h3>\n",
    "\n",
    "The goal of the `fit_ppo` function is to train the model using the Proximal Policy Optimization (PPO) loss function based on boards and returns generated from simulations. As mentioned earlier, the PPO loss function consists of three components:\n",
    "\n",
    "1. **Policy Loss**: This measures the difference between the predicted action probabilities and the action probabilities that maximize the expected return. The policy loss helps the agent to learn the optimal policy by adjusting the probabilities of taking certain actions.\n",
    "\n",
    "$$\n",
    "\\text{Policy Loss} = -\\mathbb{E}_{t} \\left[ \\min \\left( \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t, \\, \\text{clip} \\left( \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}, 1 - \\epsilon, 1 + \\epsilon \\right) \\hat{A}_t \\right) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\pi_{\\theta}$ represents the current policy.\n",
    "- $\\pi_{\\theta_{\\text{old}}}$ represents the old policy before the update.\n",
    "- $a_t$ is the action taken at time $t$.\n",
    "- $s_t$ is the state at time $t$.\n",
    "- $\\hat{A}_t$ is the advantage estimate at time $t$.\n",
    "- $\\epsilon$ is the clipping parameter.\n",
    "\n",
    "2. **Value Loss**: This measures the difference between the predicted value function and the observed returns. The value loss helps the agent to accurately estimate the value of different states, which is crucial for making informed decisions.\n",
    "\n",
    "$$\n",
    "\\text{Value Loss} = \\mathbb{E}_{t} \\left[ \\left( V_{\\theta}(s_t) - R_t \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $V_{\\theta}(s_t)$ is the predicted value function for state $s_t$.\n",
    "- $R_t$ is the observed return at time $t$.\n",
    "\n",
    "3. **Entropy Loss**: This measures the uncertainty or randomness in the agent's policy. The entropy loss encourages exploration by preventing the policy from becoming too deterministic, thus promoting a more robust learning process.\n",
    "\n",
    "$$\n",
    "\\text{Entropy Loss} = -\\mathbb{E}_{t} \\left[ \\sum_{a} \\pi_{\\theta}(a | s_t) \\log \\pi_{\\theta}(a | s_t) \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\pi_{\\theta}(a | s_t)$ is the probability of taking action $a$ in state $s_t$ under the current policy.\n",
    "\n",
    "\n",
    "4. **Total Loss**: This is the combined loss function used to update the model parameters. It incorporates the policy loss, value loss, and entropy loss, balanced by their respective coefficients.\n",
    "\n",
    "$$\n",
    "\\text{Total Loss} = \\text{Policy Loss} + c_v \\cdot \\text{Value Loss} - c_e \\cdot \\text{Entropy Loss}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $c_v$ is the value loss coefficient.\n",
    "- $c_e$ is the entropy loss coefficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d65e923-57ad-40dd-a7b8-531caca2f861",
   "metadata": {
    "id": "1d65e923-57ad-40dd-a7b8-531caca2f861"
   },
   "outputs": [],
   "source": [
    "def fit_ppo(data_loader,agents=agents,loss_dict=loss,optim=optim,device=device,scheduler=scheduler,echo=True):\n",
    "\n",
    "      agents[0].to(device).train()  # model to training mode\n",
    "      agents[1].to(device).eval()  # model to evaluation mode\n",
    "      ##############################\n",
    "      model_clone = copy.deepcopy(agents[0]) # later the old model\n",
    "      ##############################\n",
    "      agents[0].return_val = True\n",
    "      agents[1].return_val = False\n",
    "      ##############################\n",
    "      avg_total_loss = 0.0\n",
    "      avg_policy_loss = 0.0\n",
    "      avg_value_loss = 0.0\n",
    "      avg_entropy_loss = 0.0\n",
    "      \n",
    "      for ep in range(epochs_per_step):\n",
    "          optim.zero_grad() # grendents accumlation\n",
    "          for batch_index,((x,mask),act,ret) in enumerate(tqdm(data_loader)):\n",
    "              \n",
    "              X = x.to(device)\n",
    "              Mask = mask.to(device)\n",
    "              #####################\n",
    "              policy_new,values= agents[0](X,Mask) # output from the model under training\n",
    "              policy_old = agents[1](X,Mask)\n",
    "              loss,policy_loss,value_loss,entropy_loss = criterion(policy_old,policy_new,\n",
    "                                                                   act.to(device),values,ret.to(device)) # calculating ppo loss\n",
    "              loss.backward()\n",
    "              ############################\n",
    "              avg_total_loss += loss.item()\n",
    "              avg_policy_loss += policy_loss.item()\n",
    "              avg_value_loss += value_loss.item()\n",
    "              avg_entropy_loss += entropy_loss.item()  \n",
    "\n",
    "          optim.step()\n",
    "\n",
    "      num_batch = len(data_loader)\n",
    "      total_ep = epochs_per_step*num_batch # total number of epochs\n",
    "      #############################\n",
    "      loss_dict['total_loss'].append(avg_total_loss/total_ep)\n",
    "      loss_dict['policy_loss'].append(avg_policy_loss/total_ep)\n",
    "      loss_dict['value_loss'].append(avg_value_loss/total_ep)\n",
    "      loss_dict['entropy_loss'].append(avg_entropy_loss/total_ep)\n",
    "      if echo :\n",
    "          print(f\"data size : {num_batch*batch_size} states.\")\n",
    "          print(f\"Avg total loss : {loss_dict['total_loss'][-1]}, Avg policy loss : {loss_dict['policy_loss'][-1]}.\")\n",
    "          print(f\"Avg value loss : {loss_dict['value_loss'][-1]}, Avg entropy loss : {loss_dict['entropy_loss'][-1]}.\")\n",
    "\n",
    "      ########################################################\n",
    "      scheduler.step()\n",
    "      model_clone.name = \"old\"\n",
    "      agents = [agents[0],model_clone]\n",
    "      #######################################################\n",
    "      agents[0].return_val = False\n",
    "      agents[1].return_val = False # only policy head works\n",
    "\n",
    "      return agents,loss_dict,optim,scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9c251",
   "metadata": {},
   "source": [
    "<h3 style=\"color: darkblue; font-weight: bold;\">Simulation and Training Loop</h3>\n",
    "\n",
    "The simulation/training loop consists of the following steps:\n",
    "\n",
    "1. **Parallel Self-Play**: The `parallelSelfPlay` function generates multiple simulated games concurrently using GPU acceleration. It takes the current agent's weights and produces trajectories of gameplay experiences.\n",
    "  \n",
    "2. **Update Agent's Weights**: After generating trajectories from self-play, the agent's weights are updated using the collected data. The `fit_ppo` function is called to train the agent using the Proximal Policy Optimization (PPO) loss function. This involves optimizing the combined loss function to update both the policy and value networks, helping the agent to learn the optimal strategy for playing Othello.\n",
    "  \n",
    "3. **Testing**: Once the agent's weights have been updated, the agent is tested against various opponents to evaluate its performance. Testing involves playing against opponents such as a random policy, another trained agent, and the minimax algorithm with a selected depth. The test results provide insights into the agent's progress and performance improvements over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452115e1-b3e1-44f9-b5c0-995ce99f902b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "452115e1-b3e1-44f9-b5c0-995ce99f902b",
    "outputId": "f756afb6-a8a6-4183-e1ab-9fcf83dccda2"
   },
   "outputs": [],
   "source": [
    "try :\n",
    "    avg_time = 0.0\n",
    "    avg_loss = 0.0\n",
    "    # 1 => steps\n",
    "    for step in range(current_step, steps + 1):\n",
    "        start = time.time()  # start counting time for the loop\n",
    "        agents[0].to(device).eval() # simulation in cpu\n",
    "        agents[1].to(device).eval()\n",
    "        ################\n",
    "        agents[0].return_val = False\n",
    "        agents[1].return_val = False # only policy head works\n",
    "        ################\n",
    "        # generating data from self playing\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        print(f\"Step Num : {step}/{steps}.\")\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "        data = parallelSelfPlay(model=agents[0],device=device,workers=num_workers,\n",
    "                                Epsilon = calcEpsilon(step),boltzmann=True,game_per_worker=game_per_worker)\n",
    "        # training loader\n",
    "        train_loader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "        # train the model\n",
    "        print(\"\")\n",
    "        agents,loss,optim,scheduler = fit_ppo(data_loader=train_loader,agents=agents,loss_dict=loss,optim=optim,\n",
    "                                                   device=device,scheduler=scheduler)\n",
    "        agents[0].cpu().eval()\n",
    "        agents[1].cpu().eval()\n",
    "\n",
    "        avg_time += (time.time() - start) / test_frq  # counting the average time for an epoch\n",
    "\n",
    "        print(f\"time stamp : {datetime.now()}\")\n",
    "\n",
    "        if step % test_frq == 0:\n",
    "              print(\"\\ntesting...\\n\")\n",
    "              RunTests(agents[0])\n",
    "              score = Test(agents[0],best_model) # new vs old\n",
    "              print(f\"model {agents[0].name} vs {best_model.name}:  {(score[0] / (score[0] + score[1]) * 100):.04f} %.\")\n",
    "              win_trace[f\"{agents[0].name}_vs_{best_model.name}\"].append((score[0] / (score[0] + score[1]) * 100))\n",
    "              ##########################################\n",
    "              time_step.append(avg_time)\n",
    "              ###########################################\n",
    "              data = {\n",
    "                  \"win trace\": win_trace,\n",
    "                  \"total loss\": loss,\n",
    "                  \"step\": step,\n",
    "                  \"time stamp\": str(datetime.now()),\n",
    "                  \"avg time ber epoch\": time_step,\n",
    "              }\n",
    "              if win_trace[f\"{agents[0].name}_vs_{best_model.name}\"][-1] > 50.0 :\n",
    "                  best_model = copy.deepcopy(agents[0])\n",
    "                  best_model.name = \"best_model\"\n",
    "                  # save model\n",
    "                  for agent in agents:\n",
    "                      agent.save(os.path.join(path, f\"shaza_{agent.name}.pth\"))\n",
    "                      torch.save({\n",
    "                          \"optimizer_state_dict\": optim.state_dict(),\n",
    "                      }, os.path.join(path, f\"optimizer.pth\"))\n",
    "                      torch.save({\n",
    "                          \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                      }, os.path.join(path, f\"scheduler.pth\"))\n",
    "\n",
    "                  # save log\n",
    "                  np.save(os.path.join(path, f\"log.npy\"), data)\n",
    "        \n",
    "              # reset vars\n",
    "              avg_time = 0.0\n",
    "              avg_loss = 0.0\n",
    "\n",
    "except Exception as e:\n",
    "    log = {\n",
    "        \"message\" : f\"{e}\",\n",
    "        \"time stamp\": str(datetime.now()),\n",
    "    }\n",
    "    print(log)\n",
    "    np.save(os.path.join(path, f\"errorlog.npy\"),log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09be26ce",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Congratulations on completing this notebook journey! Happy coding, and may your algorithms always find success! 🚀🤖\n",
    "\n",
    "Best regards,  \n",
    "[Mohammed Yahya Yousif](https://www.linkedin.com/in/mohammed-yousif-6b272a241/)  \n",
    "GitHub: [mohammed-tech-innovator](https://github.com/mohammed-tech-innovator)  \n",
    "Website: [tech-innovator.me](https://tech-innovator.me/)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
